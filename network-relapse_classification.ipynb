{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "#device = 0\n",
    "#torch.cuda.set_device(device)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "#https://github.com/FAU-DLM/wsi_processing_pipeline\n",
    "sys.path.append(\"../wsi_processing_pipeline/\")\n",
    "import tile_extraction\n",
    "from tile_extraction import tiles, util, slide\n",
    "\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision.learner import model_meta\n",
    "\n",
    "sys.path.append('../models-pytorch/pretrained-models.pytorch')\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels import *\n",
    "\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import *\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "from functools import partial, update_wrapper\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "\n",
    "\n",
    "PATH = Path('/home/Deep_Learner/private/network/datasets/Hypophysenadenome-Rezidive/')\n",
    "PATH_LOCAL = Path('/home/Deep_Learner/private/local/')\n",
    "WSIS_RELAPSE = PATH/'wsis_relapse'\n",
    "WSIS_NON_RELAPSE = PATH/'wsis_non_relapse'\n",
    "TILES_RELAPSE = PATH/'tiles_relapse'\n",
    "TILES_NON_RELAPSE = PATH/'tiles_non_relapse'\n",
    "\n",
    "LABELS_NAME = 'rezidive.xlsx'\n",
    "LABELS = PATH/LABELS_NAME\n",
    "\n",
    "\n",
    "\n",
    "nw = 16   #number of workers for data loader\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "#def batch_stats(self, funcs:Collection[Callable]=None)->Tensor:\n",
    "#        \"Grab a batch of data and call reduction function `func` per channel\"\n",
    "#        funcs = ifnone(funcs, [torch.mean,torch.std])\n",
    "#        x = self.one_batch(ds_type=DatasetType.Train, denorm=False)[0].cpu()\n",
    "#        return [func(channel_view(x), 1) for func in funcs]\n",
    "#        \n",
    "#vision.data.ImageDataBunch.batch_stats = batch_stats\n",
    "\n",
    "sz = 512\n",
    "bs = 6\n",
    "\n",
    "#fastai defaults\n",
    "tta_beta = 0.4 \n",
    "tta_scale = 1.35\n",
    "dropout = 0.5\n",
    "wd = 0.01\n",
    "\n",
    "seed = 54\n",
    "np.random.seed(seed)\n",
    "\n",
    "num2lbs = {\n",
    "    0:\"non_relapse\", \n",
    "    1:\"relapse\"\n",
    "}\n",
    "\n",
    "lbs2num = {l:n for n,l in num2lbs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import flatten_model\n",
    "\n",
    "def arch_summary(arch):\n",
    "    model = arch(False)\n",
    "    tot = 0\n",
    "    for i, l in enumerate(model.children()):\n",
    "        n_layers = len(flatten_model(l))\n",
    "        tot += n_layers\n",
    "        print(f'({i}) {l.__class__.__name__:<12}: {n_layers:<4}layers (total: {tot})')\n",
    "\n",
    "def show(np):\n",
    "    return util.np_to_pil(np)\n",
    "\n",
    "Path.ls = lambda x: [p for p in list(x.iterdir()) if '.ipynb_checkpoints' not in p.name]\n",
    "\n",
    "def show_multiple_images(path, rows = 3, figsize=(128, 64)):\n",
    "    imgs = [open_image(p) for p in path.ls()]\n",
    "    show_all(imgs=imgs, r=rows, figsize=figsize)\n",
    "    \n",
    "def show_multiple_images_big(path:pathlib.Path):\n",
    "    for p in path.ls():\n",
    "        plt.imshow(mpimg.imread(str(p)))\n",
    "        plt.show()\n",
    "        \n",
    "def get_id_from_path(path):\n",
    "    path = Path(path)\n",
    "    split = path.stem.split('-')\n",
    "    return f'{split[0]}-{split[1]}'\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    result = []\n",
    "    for l in list_of_lists:\n",
    "        if len(l) == 1:\n",
    "            result.append(l[0])\n",
    "        else:\n",
    "            for elem in l:\n",
    "                result.append(elem)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/PPPW/deep-learning-random-explore/blob/master/CNN_archs/cnn_archs.ipynb\n",
    "\n",
    "def identity(x): return x\n",
    "\n",
    "def nasnetamobile(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.nasnetamobile(pretrained=pretrained, num_classes=1000)  \n",
    "    model.logits = identity\n",
    "    model_meta[nasnetamobile] =  { 'cut': identity, 'split': lambda m: (list(m[0][0].children())[8], m[1]) }\n",
    "    return nn.Sequential(model)\n",
    "\n",
    "#arch_summary(lambda _: nasnetamobile(False)[0])\n",
    "\n",
    "def se_resnext50_32x4d(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.se_resnext50_32x4d(pretrained=pretrained)\n",
    "    model_meta[se_resnext50_32x4d] =  {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n",
    "    return model\n",
    "\n",
    "#arch_summary(lambda _: pretrainedmodels.se_resnext50_32x4d(pretrained=None))\n",
    "\n",
    "def se_resnext101_32x4d(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.se_resnext101_32x4d(pretrained=pretrained)\n",
    "    model_meta[se_resnext101_32x4d] =  {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n",
    "    return model\n",
    "\n",
    "def xception(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.xception(pretrained=pretrained)\n",
    "    model_meta[xception] =  { 'cut': -1, 'split': lambda m: (m[0][11], m[1]) }\n",
    "    return model\n",
    "\n",
    "def inceptionv4(pretrained=True):\n",
    "    pretrained = 'imagenet' if pretrained else None\n",
    "    model = pretrainedmodels.inceptionv4(pretrained=pretrained)\n",
    "    model_meta[xception] =  { 'cut': -2, 'split': lambda m: (m[0][11], m[1]) }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#n='test'\n",
    "\n",
    "n = np.load('n-rez.npy')\n",
    "print(n)\n",
    "\n",
    "m = n+1\n",
    "m=7\n",
    "np.save('n-rez', m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create pandas dataframe with tile information to later extract tiles on the fly from WSIs during training (use this, if you do not have extracted tiles saved on disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiles_df_path = PATH/'tiles_info-tile_score_thresh=0.4-tiles.scoring_function_1.csv'\n",
    "\n",
    "\n",
    "if os.path.isfile(tiles_df_path):\n",
    "    ###\n",
    "    # just load from disc, if you have already calculated tile infos before\n",
    "    ###\n",
    "    tiles_df = pd.read_csv(tiles_df_path).set_index('tile_name')\n",
    "else:\n",
    "    ###\n",
    "    # generate and save tile info\n",
    "    ###\n",
    "    wsis_paths_relapse = [p for p in WSIS_RELAPSE.ls() if (p.suffix == '.ndpi' and '-HE' in p.name)]\n",
    "    wsis_paths_non_relapse = [p for p in WSIS_NON_RELAPSE.ls() if (p.suffix == '.ndpi' and '-HE' in p.name)]\n",
    "    wsis_paths_all = wsis_paths_relapse + wsis_paths_non_relapse\n",
    "    tiles_df = tiles.WsiOrROIToTilesMultithreaded(wsiPaths=wsis_paths_all, \n",
    "                                   tilesFolderPath=None, \n",
    "                                   tileHeight=1024, \n",
    "                                   tileWidth=1024, \n",
    "                                   tile_naming_func=tiles.get_wsi_name_from_path_pituitary_adenoma_entities, \n",
    "                                   tile_score_thresh=0.4, \n",
    "                                   tileScoringFunction=tiles.scoring_function_1, \n",
    "                                   is_wsi=True, \n",
    "                                   level=0, \n",
    "                                   save_tiles=False)\n",
    "    tiles_df.to_csv(tiles_df_path, index_label='tile_name')\n",
    "    \n",
    "tiles_df.index.name = 'tile_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wsi_path</th>\n",
       "      <th>level</th>\n",
       "      <th>x_upper_left</th>\n",
       "      <th>y_upper_left</th>\n",
       "      <th>pixels_width</th>\n",
       "      <th>pixels_height</th>\n",
       "      <th>tile_name.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tile_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024-h1024.png</th>\n",
       "      <td>/home/Deep_Learner/private/network/datasets/Hy...</td>\n",
       "      <td>0</td>\n",
       "      <td>18432</td>\n",
       "      <td>16384</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024-h1024.png</th>\n",
       "      <td>/home/Deep_Learner/private/network/datasets/Hy...</td>\n",
       "      <td>0</td>\n",
       "      <td>14336</td>\n",
       "      <td>16384</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024-h1024.png</th>\n",
       "      <td>/home/Deep_Learner/private/network/datasets/Hy...</td>\n",
       "      <td>0</td>\n",
       "      <td>16384</td>\n",
       "      <td>16384</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024-h1024.png</th>\n",
       "      <td>/home/Deep_Learner/private/network/datasets/Hy...</td>\n",
       "      <td>0</td>\n",
       "      <td>17408</td>\n",
       "      <td>16384</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024-h1024.png</th>\n",
       "      <td>/home/Deep_Learner/private/network/datasets/Hy...</td>\n",
       "      <td>0</td>\n",
       "      <td>17408</td>\n",
       "      <td>15360</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             wsi_path  \\\n",
       "tile_name                                                                                               \n",
       "1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024-...  /home/Deep_Learner/private/network/datasets/Hy...   \n",
       "1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024-...  /home/Deep_Learner/private/network/datasets/Hy...   \n",
       "1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024-...  /home/Deep_Learner/private/network/datasets/Hy...   \n",
       "1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024-...  /home/Deep_Learner/private/network/datasets/Hy...   \n",
       "1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024-...  /home/Deep_Learner/private/network/datasets/Hy...   \n",
       "\n",
       "                                                    level  x_upper_left  \\\n",
       "tile_name                                                                 \n",
       "1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024-...      0         18432   \n",
       "1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024-...      0         14336   \n",
       "1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024-...      0         16384   \n",
       "1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024-...      0         17408   \n",
       "1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024-...      0         17408   \n",
       "\n",
       "                                                    y_upper_left  \\\n",
       "tile_name                                                          \n",
       "1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024-...         16384   \n",
       "1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024-...         16384   \n",
       "1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024-...         16384   \n",
       "1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024-...         16384   \n",
       "1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024-...         15360   \n",
       "\n",
       "                                                    pixels_width  \\\n",
       "tile_name                                                          \n",
       "1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024-...          1024   \n",
       "1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024-...          1024   \n",
       "1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024-...          1024   \n",
       "1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024-...          1024   \n",
       "1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024-...          1024   \n",
       "\n",
       "                                                    pixels_height  tile_name.1  \n",
       "tile_name                                                                       \n",
       "1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024-...           1024          NaN  \n",
       "1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024-...           1024          NaN  \n",
       "1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024-...           1024          NaN  \n",
       "1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024-...           1024          NaN  \n",
       "1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024-...           1024          NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset into train, valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_excel(LABELS)\n",
    "test_pct = 0.0\n",
    "valid_pct = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = list(set([get_id_from_path(p) for p in tiles_df['wsi_path'].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = []\n",
    "for case_id in case_ids:\n",
    "    row = labels_df.loc[labels_df['case_nr'] == case_id]\n",
    "    try:\n",
    "        patients.append(row['patient_id'].values[0])\n",
    "    except:\n",
    "        print(case_id)\n",
    "        \n",
    "patients = list(set(patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_pct > 0:\n",
    "    patients_train_and_valid, patients_test = train_test_split(patients, test_size=test_pct, random_state=seed)\n",
    "else:\n",
    "    patients_test = []\n",
    "    patients_train_and_valid = patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_train, patients_valid = train_test_split(patients_train_and_valid, test_size=valid_pct, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_names_all = tiles_df.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_names_train = []\n",
    "tile_names_valid = []\n",
    "tile_names_test = []\n",
    "\n",
    "for name in tile_names_all:\n",
    "    case_id = get_id_from_path(name)\n",
    "    patient_id = labels_df.loc[labels_df['case_nr'] == case_id]['patient_id'].values[0]\n",
    "    if patient_id in patients_test:\n",
    "        tile_names_test.append(name)\n",
    "    elif patient_id in patients_valid:\n",
    "        tile_names_valid.append(name)\n",
    "    elif patient_id in patients_train:\n",
    "        tile_names_train.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiles_train_and_valid = pd.DataFrame((tile_names_train+tile_names_valid), columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name\n",
       "0  1422-10-IV-HE-tile-r17-c19-x18432-y16384-w1024...\n",
       "1  1422-10-IV-HE-tile-r17-c15-x14336-y16384-w1024...\n",
       "2  1422-10-IV-HE-tile-r17-c17-x16384-y16384-w1024...\n",
       "3  1422-10-IV-HE-tile-r17-c18-x17408-y16384-w1024...\n",
       "4  1422-10-IV-HE-tile-r16-c18-x17408-y15360-w1024..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiles_train_and_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### obsolete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#obsolete\n",
    "\n",
    "#df = pd.read_excel(LABELS).set_index('id')\n",
    "#test_pct = 0\n",
    "#valid_pct = 0.15\n",
    "#\n",
    "####\n",
    "## RELAPSE\n",
    "####\n",
    "#\n",
    "##key: patient, value: list of wsi names\n",
    "#patient_to_wsi_ids_relapse = {}\n",
    "#\n",
    "#\n",
    "####\n",
    "## Option 1: use this, if you already have extracted tiles saved to disc\n",
    "####\n",
    "##ids_relapse_all = [get_id_from_path(p) for p in (WSIS_RELAPSE.ls()) if p.suffix == '.ndpi']\n",
    "#\n",
    "####\n",
    "## Option 2: use this, if you only have a dataframe generated in 5.1\n",
    "####\n",
    "#ids_relapse_all = list(set([get_id_from_path(p) for p in tiles_df_relapse.index.tolist()]))\n",
    "#\n",
    "#\n",
    "#excluded_ids = []\n",
    "#for id in ids_relapse_all:\n",
    "#    if id not in excluded_ids:\n",
    "#        patient = df.at[id, 'Patient']\n",
    "#        if patient in patient_to_wsi_ids_relapse.keys():\n",
    "#            patient_to_wsi_ids_relapse[patient].append(id)\n",
    "#        else:\n",
    "#            patient_to_wsi_ids_relapse[patient] = [id]\n",
    "#if test_pct != 0:            \n",
    "#    patients_relapse_train_and_valid, patients_relapse_test = train_test_split(list(patient_to_wsi_ids_relapse.keys()), \n",
    "#                                                                               test_size=test_pct, \n",
    "#                                                                               random_state=seed)\n",
    "#    patients_relapse_train, patients_relapse_valid = train_test_split(patients_relapse_train_and_valid, \n",
    "#                                                                      test_size=valid_pct, \n",
    "#                                                                      random_state=seed)\n",
    "#else:\n",
    "#    patients_relapse_train, patients_relapse_valid = train_test_split(list(patient_to_wsi_ids_relapse.keys()), \n",
    "#                                                                      test_size=valid_pct, \n",
    "#                                                                      random_state=seed)\n",
    "#    patients_relapse_test = []\n",
    "#    \n",
    "#    \n",
    "#\n",
    "#ids_relapse_train = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_train])\n",
    "#ids_relapse_valid = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_valid])\n",
    "#ids_relapse_test = flatten([patient_to_wsi_ids_relapse[pat] for pat in patients_relapse_test])\n",
    "#\n",
    "#tile_paths_relapse_all = [Path(f'{p.parts[-2]}/{p.parts[-1]}') for p in (TILES_RELAPSE.ls()) if p.suffix == '.png']\n",
    "#tile_paths_relapse_train = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_train]\n",
    "#tile_paths_relapse_val = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_valid]\n",
    "#tile_paths_relapse_test = [p for p in tile_paths_relapse_all if get_id_from_path(p) in ids_relapse_test]\n",
    "#\n",
    "####\n",
    "## NON RELAPSE\n",
    "####\n",
    "#\n",
    "####\n",
    "## Option 1: use this, if you already have extracted tiles saved to disc\n",
    "####\n",
    "##tile_paths_non_relapse_all = [Path(f'{p.parts[-2]}/{p.parts[-1]}') for p in (TILES_NON_RELAPSE.ls()) if p.suffix == '.png']\n",
    "#\n",
    "####\n",
    "## Option 2: use this, if you only have a dataframe generated in 5.1\n",
    "####\n",
    "#tile_paths_non_relapse_all = list(set([Path(p) for p in tiles_df_non_relapse.index.tolist()]))\n",
    "#\n",
    "#\n",
    "#ids_non_relapse_all = []\n",
    "#for p in tqdm(tile_paths_non_relapse_all):\n",
    "#    ids_non_relapse_all.append(get_id_from_path(p))\n",
    "#ids_non_relapse_all = list(set(ids_non_relapse_all))\n",
    "#\n",
    "#if test_pct != 0:\n",
    "#    ids_non_relapse_train_and_valid, ids_non_relapse_test = train_test_split(ids_non_relapse_all, \n",
    "#                                                                             test_size=test_pct, \n",
    "#                                                                             random_state=seed)\n",
    "#    ids_non_relapse_train, ids_non_relapse_val = train_test_split(ids_non_relapse_train_and_valid, \n",
    "#                                                                  test_size=valid_pct, \n",
    "#                                                                  random_state=seed)\n",
    "#else:\n",
    "#    ids_non_relapse_train, ids_non_relapse_val = train_test_split(ids_non_relapse_all, \n",
    "#                                                                  test_size=valid_pct, \n",
    "#                                                                  random_state=seed)\n",
    "#    ids_non_relapse_test = []\n",
    "#    \n",
    "#\n",
    "#tile_paths_non_relapse_train = [p for p in tile_paths_non_relapse_all if get_id_from_path(p) in ids_non_relapse_train]\n",
    "#tile_paths_non_relapse_val = [p for p in tile_paths_non_relapse_all if get_id_from_path(p) in ids_non_relapse_val]\n",
    "#tile_paths_non_relapse_test = [p for p in tile_paths_non_relapse_all if get_id_from_path(p) in ids_non_relapse_test]\n",
    "#\n",
    "####\n",
    "## COMBINE\n",
    "####\n",
    "#tile_paths_train = tile_paths_non_relapse_train + tile_paths_relapse_train\n",
    "#tile_paths_val = tile_paths_non_relapse_val + tile_paths_relapse_val\n",
    "#tile_paths_test = tile_paths_non_relapse_test + tile_paths_relapse_test\n",
    "#\n",
    "#df_tile_paths_train_and_valid = pd.DataFrame((tile_paths_train+tile_paths_val), columns=['name'])\n",
    "#\n",
    "#print(f'seed: {seed}')\n",
    "#print(len(tile_paths_train))\n",
    "#print(len(set([get_id_from_path(p) for p in tile_paths_train])))\n",
    "#print(len(tile_paths_val))\n",
    "#print(len(set([get_id_from_path(p) for p in tile_paths_val])))\n",
    "#print(len(tile_paths_test))\n",
    "#print(len(set([get_id_from_path(p) for p in tile_paths_test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(flip_vert=True)\n",
    "\n",
    "#tfms = ([RandTransform(tfm=TfmAffine (dihedral_affine), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True),\n",
    "#        RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.475, 0.525)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "#        RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.95, 1.0526315789473684)}, p=0.75, resolved={}, do_run=True, is_random=True)],\n",
    "#        [])\n",
    "\n",
    "#def get_ex(): return open_image(str(TRAIN.ls()[0]))\n",
    "#\n",
    "#def plots_f(rows, cols, width, height, **kwargs):\n",
    "#    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n",
    "#        rows,cols,figsize=(width,height))[1].flatten())]\n",
    "#\n",
    "#plots_f(2, 4, 12, 6, size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# if you use a pandas dataframe generated in 5.1 to extract tiles on the fly during training, \n",
    "# overwrite fastai.vision.data.ImageList.open and fastai.vision.image.open_image\n",
    "###\n",
    "def open_custom(self, fn):\n",
    "    \"Open image in `fn`.\"\n",
    "    return open_image_custom(fn, convert_mode=self.convert_mode, after_open=self.after_open)\n",
    "\n",
    "def open_image_custom(fn:PathOrStr, \n",
    "                      div:bool=True, \n",
    "                      convert_mode:str='RGB', \n",
    "                      cls:type=fastai.vision.Image, \n",
    "                      after_open:Callable=None)->fastai.vision.Image:\n",
    "        \"Open image in `fn`.\"\n",
    "        fn = Path(fn)\n",
    "        tile_name = fn.name\n",
    "        row = tiles_df.loc[tile_name, : ]\n",
    "        wsi_path = row['wsi_path']\n",
    "        x = row['x_upper_left']\n",
    "        y = row['y_upper_left']\n",
    "        width = row['pixels_width']\n",
    "        height = row['pixels_height']\n",
    "        level = row['level']\n",
    "        tile = tiles.ExtractTileFromWSI(path=wsi_path, x=x, y=y, width=width, height=height, level=level)\n",
    "        tile = tile.convert(convert_mode)\n",
    "        if after_open: \n",
    "            tile = after_open(tile)\n",
    "        tile = pil2tensor(tile,np.float32)\n",
    "        if div: \n",
    "            tile.div_(255)\n",
    "        return cls(tile)\n",
    "        \n",
    "fastai.vision.data.ImageList.open = open_custom\n",
    "fastai.vision.image.open_image = open_image_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(path):\n",
    "    path = Path(path)\n",
    "    case_id = get_id_from_path(path)\n",
    "    lbl = labels_df.loc[labels_df['case_nr'] == case_id]['relapse (0=no; 1=yes)'].values[0]\n",
    "    return [int(lbl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_func(path):\n",
    "    return str(Path(path).name) in tile_names_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = ImageList.from_folder(path=TRAIN, extensions=['.png'])\n",
    "data = ImageList.from_df(df_tiles_train_and_valid, path='')\n",
    "data = data.split_by_valid_func(split_func)\n",
    "data = data.label_from_func(label_func)\n",
    "data = data.transform(tfms=tfms, size=sz)\n",
    "#data = data.add_test_folder(test_folder=TEST_EXPERIMENTING)\n",
    "if test_pct > 0:\n",
    "    data = data.add_test([PATH/p for p in tile_paths_test])\n",
    "data = data.databunch(bs=bs, num_workers=nw, path=PATH/f'{n}-currently-training')\n",
    "data = data.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (33469 items)\n",
       "x: ImageList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: MultiCategoryList\n",
       "1,1,1,1,1\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (1119 items)\n",
       "x: ImageList\n",
       "Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512),Image (3, 512, 512)\n",
       "y: MultiCategoryList\n",
       "1,1,1,1,1\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_frozen = 5\n",
    "epochs_unfrozen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnext101_32x8d\n",
    "learner = cnn_learner(data=data, \n",
    "                     base_arch=arch, \n",
    "                     metrics=[accuracy_thresh], \n",
    "                     ps=dropout, \n",
    "                     pretrained=True, \n",
    "                     wd = wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7-resnext101_32x8d-size512-bs6-epochs_head5-epochs_complete10-seed_54-test_pct_0.0-valid_pct_0.15'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameBase = f'{n}-{arch.__name__}-size{sz}-bs{bs}-epochs_head{epochs_frozen}-epochs_complete{epochs_unfrozen}-seed_{seed}-test_pct_{test_pct}-valid_pct_{valid_pct}'\n",
    "nameBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.695310</td>\n",
       "      <td>0.642091</td>\n",
       "      <td>1:59:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210833</td>\n",
       "      <td>0.340027</td>\n",
       "      <td>0.882931</td>\n",
       "      <td>42:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129852</td>\n",
       "      <td>0.626837</td>\n",
       "      <td>0.721626</td>\n",
       "      <td>42:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162826</td>\n",
       "      <td>0.384088</td>\n",
       "      <td>0.843610</td>\n",
       "      <td>42:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.149070</td>\n",
       "      <td>0.535497</td>\n",
       "      <td>0.755585</td>\n",
       "      <td>42:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(cyc_len=epochs_frozen, max_lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameHead = f'{nameBase}-head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(nameHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load('bestmodel_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.lr_find(start_lr=1e-10, end_lr=10, num_it=1000)\n",
    "learner.recorder.plot(skip_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = 2e-6\n",
    "lr3 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.108574</td>\n",
       "      <td>0.467673</td>\n",
       "      <td>0.789991</td>\n",
       "      <td>1:03:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.103077</td>\n",
       "      <td>0.585406</td>\n",
       "      <td>0.751117</td>\n",
       "      <td>1:03:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.091265</td>\n",
       "      <td>0.297785</td>\n",
       "      <td>0.871314</td>\n",
       "      <td>1:03:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.097935</td>\n",
       "      <td>0.349599</td>\n",
       "      <td>0.848972</td>\n",
       "      <td>1:03:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062881</td>\n",
       "      <td>0.380134</td>\n",
       "      <td>0.836461</td>\n",
       "      <td>1:03:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.064734</td>\n",
       "      <td>0.537016</td>\n",
       "      <td>0.789544</td>\n",
       "      <td>1:03:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.377421</td>\n",
       "      <td>0.850760</td>\n",
       "      <td>1:03:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.063075</td>\n",
       "      <td>0.433072</td>\n",
       "      <td>0.832887</td>\n",
       "      <td>1:03:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048976</td>\n",
       "      <td>0.589960</td>\n",
       "      <td>0.790438</td>\n",
       "      <td>1:03:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.510253</td>\n",
       "      <td>0.810098</td>\n",
       "      <td>1:03:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(cyc_len=epochs_unfrozen, \n",
    "                      max_lr=slice(lr2, lr3))\n",
    "#learner.fit_one_cycle(cyc_len=epochs_unfrozen, \n",
    "                      #max_lr=slice(lr2, lr3), \n",
    "                      #callbacks=[SaveModelCallback(learner, every='epoch', monitor='accuracy_thresh')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameComplete = f'{nameBase}-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(nameComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load(nameComplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction per case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(predicted_classes:list, all_classes:list):\n",
    "    for c in predicted_classes:\n",
    "        assert c in all_classes\n",
    "    n = len(all_classes)\n",
    "    res = np.zeros(n, int)\n",
    "    for i, c in enumerate(all_classes):\n",
    "        if c in predicted_classes:\n",
    "            res[i] = 1 \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def ensemble_predict(dict_arch_to_path_of_saved_model:typing.Dict[Callable, pathlib.Path], \n",
    "                     data:fastai.vision.data.ImageDataBunch,\n",
    "                     ds_type:fastai.basic_data.DatasetType,\n",
    "                     tta:bool, \n",
    "                     scale:float,\n",
    "                     beta:float):\n",
    "    \"\"\"\n",
    "    tta: Should test time augmentation be used?\n",
    "    scale: if tta is True -> scaling factor for tta\n",
    "    beta: if tta is True -> beta factor for tta\n",
    "    check this out for more infos: https://docs.fast.ai/basic_train.html#Test-time-augmentation\n",
    "    \"\"\"\n",
    "   \n",
    "    print(f'{str([a.__name__ for a in dict_arch_to_path_of_saved_model.keys()])}_sz{sz}_ensembled')\n",
    "    \n",
    "    predsList = []\n",
    "    for arch in dict_arch_to_path_of_saved_model.keys():\n",
    "        learner = cnn_learner(data=data, base_arch=arch, pretrained=False)\n",
    "        learner.load(dict_arch_to_path_of_saved_model[arch])\n",
    "        if tta is True:\n",
    "            preds = learner.TTA(beta=beta, scale=scale, ds_type=ds_type)\n",
    "        else:\n",
    "            preds = learner.get_preds(ds_type=ds_type)\n",
    "            \n",
    "        predsList.append(preds)\n",
    "    \n",
    "    preds_ensembled = predsList[0]\n",
    "    for n, _ in enumerate(predsList):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        else:\n",
    "            preds_ensembled[0] = preds_ensembled[0] + predsList[n][0]\n",
    "    preds_ensembled[0] = preds_ensembled[0]/len(predsList)\n",
    "    \n",
    "    return preds_ensembled\n",
    "\n",
    "def from_preds_to_dict_path_to_preds(preds, \n",
    "                                     imageDataBunch:fastai.vision.ImageDataBunch, \n",
    "                                     ds_type:fastai.basic_data.DatasetType,\n",
    "                                     threshold:float):\n",
    "    \"\"\"\n",
    "    preds: What fastai.vision.learner.get_preds or fastai.vision.learner.TTA return.\n",
    "            two tensors: 1st: lists with raw predictions for each class of an image\n",
    "                         2nd: lists with y_true\n",
    "            form e.g. [tensor([[0.9672, 0.9211, 0.4560, 0.8185], \n",
    "                                [0.9498, 0.8600, 0.5852, 0.7206]]),\n",
    "                         tensor([[0., 0., 0., 1.],\n",
    "                                [0., 0., 1., 1.]])]\n",
    "                                \n",
    "    RETURN:\n",
    "        key:path, value:tuple (fastai.core.MultiCategory, tensor preds one hot encoded, tensor with pure preds) \n",
    "        e.g. (MultiCategory 0, tensor([1., 0., 0., 0.]), tensor([0.9952, 0.0015, 0.0021, 0.0029]))\n",
    "    path_to_pred = {}\n",
    "    \"\"\"\n",
    "    #key:path, value:tuple (fastai.core.MultiCategory, tensor preds one hot encoded, tensor with pure preds) \n",
    "    #e.g. (MultiCategory 0, tensor([1., 0., 0., 0.]), tensor([0.9952, 0.0015, 0.0021, 0.0029]))\n",
    "    path_to_pred = {}\n",
    "    d = None\n",
    "    if ds_type is DatasetType.Valid:\n",
    "        d = imageDataBunch.valid_ds\n",
    "    elif ds_type is DatasetType.Test:\n",
    "        d = imageDataBunch.test_ds\n",
    "    elif ds_type is DatasetType.Train:\n",
    "        d = imageDataBunch.train_ds\n",
    "    for path, pred in tqdm(zip(d.items, preds[0]), total = len(d.items)):\n",
    "        multi_c = None\n",
    "        pred_one_hot_encoded = (pred > threshold).float()\n",
    "        pred_raw = pred\n",
    "        path_to_pred[path] = multi_c, pred_one_hot_encoded, pred_raw\n",
    "        \n",
    "    return path_to_pred\n",
    "\n",
    "\n",
    "def get_class_occurence_per_id(learner:fastai.vision.learner=None,\n",
    "                               labelList:fastai.data_block.LabelList=None,\n",
    "                               dict_arch_to_path_of_saved_model:typing.Dict[Callable, pathlib.Path]=None,\n",
    "                               imageDataBunch:fastai.vision.data.ImageDataBunch=None,\n",
    "                               ds_type:fastai.basic_data.DatasetType=None,\n",
    "                               tta:bool=False,                                          \n",
    "                               threshold = 0.5,                              \n",
    "                               scale:float = 1.35,\n",
    "                               beta: float = 0.4):\n",
    "    \"\"\"\n",
    "    Option 1: Hand over a fastai.vision.learner and fastai.data_block.LabelList. No tta and no ensembling available\n",
    "                for this option.\n",
    "    Option 2: Hand over a fastai.vision.learner that was initalized with a fastai.vision.data.ImageDataBunch object.\n",
    "    Option 3: Hand over dict where the keys are functions to create a model (e.g. torchvision.models.resnet50)\n",
    "                and the values are paths to saved weights. Do this to use ensembling.\n",
    "    \n",
    "    Params:\n",
    "        threshold:  threshold to consider the predictions to be correct or not\n",
    "        scale: only needed when tta is True; scale value for fastai's fastai.basic_train.Learner.TTA function\n",
    "        beta: only needed when tta is True; beta value for fastai's fastai.basic_train.Learner.TTA function\n",
    "    \"\"\"\n",
    "    \n",
    "    if labelList is not None and ds_type is not None:\n",
    "        raise ValueError('One of dataset or ds_type must be None')\n",
    "    if labelList is not None and tta is True:\n",
    "        raise ValueError('TTA is not available for a custom LabelList')\n",
    "                \n",
    "    #key:path, value:tuple (fastai.core.MultiCategory, tensor preds one hot encoded, tensor with pure preds) \n",
    "    #e.g. (MultiCategory 0, tensor([1., 0., 0., 0.]), tensor([0.9952, 0.0015, 0.0021, 0.0029]))\n",
    "    path_to_pred = {}\n",
    "    \n",
    "    #Option 1\n",
    "    if learner is not None and labelList is not None:\n",
    "        for n, path in tqdm(enumerate(labelList.items), total=len(labelList.items)):\n",
    "            pred = learner.predict(labelList[n][0], thresh=threshold)\n",
    "            path_to_pred[path] = pred\n",
    "    \n",
    "    #Option 2\n",
    "    elif learner is not None and labelList is None and  not dict_arch_to_path_of_saved_model and imageDataBunch is None:\n",
    "        if tta is True:\n",
    "            preds = learner.TTA(beta=beta, scale=scale, ds_type=ds_type)\n",
    "        else:\n",
    "            preds = learner.get_preds(ds_type=ds_type)\n",
    "        path_to_pred = from_preds_to_dict_path_to_preds(preds, learner.data, ds_type, threshold)\n",
    "                \n",
    "    #Option 3\n",
    "    elif dict_arch_to_path_of_saved_model and imageDataBunch is not None:\n",
    "        preds = ensemble_predict(dict_arch_to_path_of_saved_model, imageDataBunch, ds_type, tta, scale, beta)\n",
    "        path_to_pred = from_preds_to_dict_path_to_preds(preds, imageDataBunch, ds_type, threshold)                \n",
    "               \n",
    "    #key: id of a case; value: list with this syntax  \n",
    "    #[<number of tiles>, \n",
    "    #[<number of occurence of class1 over all tiles per id>, \n",
    "    #<number of occurence of class2 over all tiles per id>, ..., \n",
    "    #<number of occurence of classN over all tiles per id>],\n",
    "    #y_true]\n",
    "    class_occurence_per_id = {}\n",
    "    \n",
    "    for path, pred in path_to_pred.items():   \n",
    "        id = get_id_from_path(path)\n",
    "        if id in class_occurence_per_id:\n",
    "            v = class_occurence_per_id[id]\n",
    "            v[0] = v[0] + 1\n",
    "            v[1] = v[1] + pred[1]\n",
    "            class_occurence_per_id[id] = v\n",
    "        else:\n",
    "            class_occurence_per_id[id] = [1, pred[1], one_hot_encode(label_func(path), lbs2num.values())]\n",
    "            \n",
    "    return class_occurence_per_id\n",
    "\n",
    "\n",
    "def get_preds_threshold_per_id(thresholds_per_class:list, class_occurence_per_id:dict):\n",
    "    #key: id of a case; \n",
    "    #value: list with this syntax  \n",
    "    #[y_pred_th e.g. [True,False,False,False], \n",
    "    #y_true e.g. [1,0,0,0]]\n",
    "    result = {}\n",
    "    for k in class_occurence_per_id.keys():\n",
    "        y_pred_th = []\n",
    "        for n, i in enumerate(class_occurence_per_id[k][1]):\n",
    "            i = int(i)\n",
    "            y_pred_th.append(i/class_occurence_per_id[k][0] > thresholds_per_class[n])\n",
    "    \n",
    "        result[k] = [y_pred_th, class_occurence_per_id[k][2]]\n",
    "    return result\n",
    "\n",
    "def get_accuracy_over_all_ids(number_of_ids, preds_threshold_per_id:dict, per_class:bool = True, number_of_classes = len(lbs2num)):\n",
    "    if per_class is True:\n",
    "        correctly_predicted = np.zeros(number_of_classes, dtype=np.int)\n",
    "    else:\n",
    "        correctly_predicted = 0\n",
    "    for k in preds_threshold_per_id.keys():\n",
    "        pred = preds_threshold_per_id[k][0]\n",
    "        true = preds_threshold_per_id[k][1]\n",
    "        for i in range(number_of_classes):\n",
    "            if true[i] == pred[i]:\n",
    "                if per_class is True:\n",
    "                    correctly_predicted[i] = correctly_predicted[i] + 1\n",
    "                else:\n",
    "                    correctly_predicted = correctly_predicted + 1\n",
    "    if per_class is True:                    \n",
    "        correctly_predicted_percentage = {}\n",
    "        for lb, num in zip(lbs2num.keys(), correctly_predicted):\n",
    "            correctly_predicted_percentage[lb] = num/number_of_ids\n",
    "    if per_class is False:\n",
    "        correctly_predicted_percentage = correctly_predicted/number_of_ids\n",
    "\n",
    "    return correctly_predicted_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arches = {resnext101_32x8d:Path(MODEL_PATH/'6-resnext101_32x8d-size512-bs8-seed_73/bestmodel_15'),\n",
    "         # se_resnext101_32x4d:MODEL_PATH/'11-se_resnext101_32x4d-size512-bs10-epochs_head5-epochs_complete5-seed_73/11-se_resnext101_32x4d-size512-bs8-epochs_head5-epochs_complete5-seed_73-complete'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = [0.5,0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copi_val = get_class_occurence_per_id(dict_arch_to_path_of_saved_model=arches,\n",
    "#                                      imageDataBunch=data,\n",
    "#                                      ds_type=DatasetType.Valid)\n",
    "copi_val = get_class_occurence_per_id(learner=learner, ds_type=DatasetType.Valid)\n",
    "preds_th_val = get_preds_threshold_per_id(ths, copi_val)\n",
    "accuracy_per_class_val = get_accuracy_over_all_ids(len(preds_th_val), preds_th_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copi_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_th_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#copi_test = get_class_occurence_per_id(dict_arch_to_path_of_saved_model=arches,\n",
    "#                                      imageDataBunch=data,\n",
    "#                                      ds_type=DatasetType.Test)\n",
    "copi_test = get_class_occurence_per_id(learner=learner, ds_type=DatasetType.Test)\n",
    "preds_th_test = get_preds_threshold_per_id(ths, copi_test)\n",
    "accuracy_per_class_test = get_accuracy_over_all_ids(len(preds_th_test), preds_th_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_per_class_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_confusion_matrix(self, slice_size:int=1):\n",
    "        \"Confusion matrix as an `np.ndarray`.\"\n",
    "        x=torch.arange(0,self.data.c)\n",
    "        if slice_size is None: cm = ((self.pred_class==x[:,None]) & (self.y_true==x[:,None,None])).sum(2)\n",
    "        else:\n",
    "            cm = torch.zeros(self.data.c, self.data.c, dtype=x.dtype)\n",
    "            for i in range(0, self.y_true.shape[0], slice_size):\n",
    "                #cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            #& (self.y_true[i:i+slice_size]==x[:,None,None])).sum(2)\n",
    "                cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            & (self.y_true[i:i+slice_size]==(x[:,None,None]).float())).sum(2)\n",
    "                torch.add(cm, cm_slice, out=cm)\n",
    "        return to_np(cm)\n",
    "    \n",
    "fastai.train.ClassificationInterpretation.confusion_matrix = custom_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM Py3",
   "language": "python",
   "name": "dlm_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "293px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
